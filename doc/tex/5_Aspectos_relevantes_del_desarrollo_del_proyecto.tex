\capitulo{5}{Aspectos relevantes del desarrollo del proyecto}

En capítulo de la memoria pretende mostrar y resumir los aspectos relevantes durante el desarrollo del proyecto. En el se incluyen decisiones tomadas durante el desarrollo, modelado del problema y resultados de los experimentos.

En la subsección~\ref{Fase de experimentación} se explica y desarrolla la fase de experimentación con modelos. Para ello se expone como se ha realizado la extracción de características, que algoritmos se han utilizado durante la experimentación, como se ha realizado el tratamiento de los datos y como se ha realizado el entrenamiento y la evaluación de los modelos.

En la subsección~\ref{Resultados} se exponen los resultados mediante gráficas y se expone el análisis realizado para seleccionar el modelo que mejor se adecúa al problema.

Por último en la subsección~\ref{Fase de desarrollo de la aplicación} se exponen detalles de la fase de desarrollo entre los que se incluyen el motivo de las tecnologías utilizadas y problemas durante el desarrollo.

\section{Fase de experimentación}
\label{Fase de experimentación}
\subsection{Extracción de características mediante un Trabajo de Fin de Grado previo}

Para la extracción de características se hizo uso de un apartado del Trabajo de Fin de Grado de Catalin llamado PADDEL~\cite{paddelRepo}. Este TFG consiste en detectar mediante la bradicenesia si una persona tiene o no Parkinson. Para ello extrae las características de vídeos haciendo el RFTT mediante visión artificial y, aplicando técnicas de minería de datos, es capaz de predecir si una persona tiene o no la enfermedad de Parkinson.

Realizando un estudio del TFG de Catalin se pudo extraer y utilizar correctamente el código que permitía la extracción de características. Este código, tras algunas modificaciones, ha sido implementado en el proyecto.

Las características que se extraen de los vídeos, se pueden dividir en tres grupos:
\begin{itemize}
\item Atributos iniciales: son los atributos que de los que se dispone mediante el titulo del vídeo como son la edad, el sexo o la mano dominante.
\item Atributos extraídos del vídeo: son los atributos que extrae mediante series temporales como son la velocidad, la amplitud o el número de golpes.
\item Atributos extraídos de vídeo con TSFresh: Son los 794 atributos
del vídeo extraídos mediante TSFresh.
\end{itemize}


\subsection{Análisis y tratamiento de los datos}
Una vez se extrajeron las características de los datos se pudo llevar a cabo un análisis de los datos.
En el conjunto de datos disponible en la fase de experimentación se disponía de 222 vídeos. 
Cada video disponía de los siguientes atributos:
\begin{itemize}
\item Sexo de la persona.
\item Edad de la persona.
\item Mano dominante de la persona.
\item Mano que aparece en el vídeo.
\item Fecha en la que se ha tomado.
\item Valoración del grado de lentitud del vídeo. Primera variable objetivo.
\item Valoración del grado de amplitud del vídeo. Segunda variable objetivo.
\end{itemize}
Las valoraciones fueron facilitadas por dos neurólogas del HUBU. Estas valoraciones diferían  entre sí de manera considerable y es por ello se decidió realizar el experimento utilizando solo las clasificaciones de una de las dos. 

Una vez analizados los datos y extraídas las características se hicieron varías comprobaciones. Primero se comprobó que todos los vídeos estuviesen correctamente valorados. Este no fue el caso ya que 2 de los videos de los que se disponía no estaban valorados es por ello que, debido a que el tipo de aprendizaje utilizado es supervisado, se decidió que no se incluirían en la fase de experimentación.

Tras eliminar los vídeos que no fueron valorados, se comprobó si las clases estaban desbalanceadas. Al realizar esta comprobación se vio que había un desbalanceado muy elevado, habiendo clases que solo tenían menos de 8 instancias. Dado que se pretendía utilizar algoritmos como KNN o técnicas de remuestreo como \textit{Random Oversampling} se tomo la decisión de eliminar aquellas clases que tuviesen menos de 8 instancias. Las clases que se eliminaron fueron lentitud 3 y 4 así como amplitud 4.
\imagen{./graphs/class_graph1.png}{Número de instancias por clase antes de realizar el tratamiento de datos.}{}
Además de lo anteriormente mencionado se comprobó si existía alguna característica de los vídeos que fuese nula. Al comprobarlo se observo que en algunas todas había una característica que era nula por lo que se decidió eliminar esa característica nula de todas las instancias ya que no aportaba ninguna información adicional. Una de esas características era la edad, que solo fue nula en vídeos de control, es decir vídeos de personas sin la enfermedad de Parkinson y dado al elevado número de vídeos de control, no supuso una gran perdida de información. Posteriormente se eliminaron aquellos vídeos cuya duración no era superior a 15 segundos ya que, según aparece en la documentación de PADDEL, no se realizaba correctamente la extracción de características si el vídeo no cumplía ese requisito.
\imagen{./graphs/class_graph1.png}{Número de instancias por clase tras realizar el tratamiento de datos.}{}
Tras eliminar las instancias anteriormente mencionadas, se realizó un tratamiento de datos en el que las característica categóricas se transformaron en numéricas para poder entrenar correctamente los modelos.

\subsection{Elección del modelo}

Una vez se trataron los datos correctamente, se realizó una selección de los modelos a evaluar. Para ello se eligieron aquellos modelos que mejor funcionaron en las pruebas de PADDEL así como los que obtuvieron mejores resultados en el artículo <<Computer Vision for Parkinson’s Disease Evaluation: A Survey on Finger Tapping>>~\cite{AmoSalas2024}. Los algoritmos seleccionados fueron KNN, \textit{Random Forest}, XGBoost y SVM.

Para la comparación de modelos, se utilizo \textit{GridSearchCV}, una técnica de validación cruzada que nos permite evaluar los mejores hiperparámetros para un modelo. Para ello el modelo se entrena utilizando una parte del \textit{dataset} como entrenamiento y la restante como prueba. Para estas pruebas se decidió utilizar 320 características de las 807 resultantes tras el tratamiento de los datos. Este dato fue extraído de la documentación del proyecto de PADDEL~\cite{paddelRepo} y probado durante los experimentos viendo que era el que mejor resultados reportaba (ver comparativa en las figuras~\ref{fig:./graphs/comparativeLentSmote.png} y~\ref{fig:./graphs/comparativeLentSmote320.png}). Es importante destacar que para entrenar cada modelos, las estadísticas que se seleccionan difieren ya que no tienen las mismas correlaciones.
\imagen{./graphs/comparativeLentSmote.png}{Resultados de modelos con todas las características y SMOTE}{}
Durante la fase de entrenamiento se hizo uso tanto de \textit{Random Over-Sampling} (ROS) como de \textit{Synthetic Minority Over-sampling Technique} (SMOTE). Estas técnicas son esenciales en problemas de desequilibrio de clases, ya que permiten entrenar el modelo haciendo que todas las clases tengan el mismo número de instancias y evitar que el modelo ignore las clases minoritarias y pueda predecirlas adecuadamente. Al utilizar este tipo de técnicas hay que tener en cuenta que solo se ha de hacer remuestreo sobre los datos de entrenamiento ya que los datos de test no se deben modificar. 

Esto supuso un problema ya que internamente \textit{GridSearchCV} no te permite separar la parte de entrenamiento y test y hacer el \textit{Over-Sampling} correctamente. Es por ello que se decidió hacer de manera externa al entrenamiento del modelo una validación cruzada estratificada dividida en 5 grupos. En ella 4 de los 5 grupos eran utilizados para entrenar el modelo y el restante se utilizaba para evaluar el rendimiento del modelo con los datos no modificados.

Dado que este problema se componía de dos variables objetivo, se opto por realizar modelos independientes para cada clase de manera que cada modelo predijese una variable. Es por ello que, durante el entrenamiento de cada una de las variable objetivo, la variable que no se predecía se omitía y no se introducía durante los entrenamientos.

\section{Resultados}
\label{Resultados}

Tras realizar el tratamiento de los datos y el entrenamiento de los modelos se obtuvieron 20 grupos de evaluaciones para cada modelo. Estos grupos están compuestos de las siguientes evaluaciones:
\begin{itemize}
\item \textit{Accuracy}
\item \textit{F-score}
\item \textit{G-mean}
\end{itemize}
Cada modelo fue evaluado con cinco veces, una por cada grupo dentro de las divisiones de la validación cruzada y dentro de cada grupo fue evaluado utilizando ROS y SMOTE.

Para realizar acabo estas evaluaciones, se utilizó tanto en la medida F-score como en la \textit{G-mean} el parámetro de \textit{average} en \textit{weighted} debido al gran desbalanceo entre clases ya que, si se hubiese indicado \textit{macro}, debido a que internamente realiza una media aritmética entre cada \textit{F-score} de cada clase, no se habría tenido en cuenta el desbalanceo.

Para obtener una información más general sobre los modelos y sus resultados se decidió que las gráficas que representaran los valores fuesen diagramas de cajas y bigotes, lo cual permitió un mejor visión general del cada modelo.

\subsection{Lentitud}
Para el caso de la lentitud, se hicieron de forma paralela la evaluación del modelo para la lentitud utilizando SMOTE y ROS (ver figuras~\ref{fig:./graphs/comparativeLentSmote320.png} y~\ref{fig:./graphs/comparativeLentRos320.png} respectivamente).
\imagen{./graphs/comparativeLentSmote320.png}{Resultados de modelos con 320 características y SMOTE.}{}
\imagen{./graphs/comparativeLentRos320.png}{Resultados de modelos con 320 características y ROS.}{}
Para hacer las comparativas entre los modelos, se omitió la \textit{Accuracy} ya que, en una clasificación multiclase, no es una buena medida para indicar como de bueno es el modelo. Es por esto que las medidas utilizadas para la comparación entre los modelos fueron el \textit{F-score} y \textit{G-mean}. No obstante, no se descartó la medida de \textit{Accuracy} durante la experimentación ya que proveía una vista general del rendimiento de los modelos frente a un problema desconocido.

En términos generales se puede observar que SMOTE tiene mejor desempeño que ROS. Tras descartar ROS, la comparación entre los modelos muestra que el algoritmo de KNN tiene un desempeño peor comparado con respecto al resto de modelos. Es por ello que queda descartado como modelo candidato. Entre los modelos que quedan \textit{XGBoost} y \textit{Random Forest} destacan sobre el SVM, descartandolo así como modelo candidato. Por último, entre los dos modelos restantes, \textit{XGBoost} pese a tener una mediana peor, posee un valor máximo que destaca en gran medida con respecto a \textit{Random Forest}. Es por ello que se decidió utilizar \textit{XGBoost} como modelo para la lentitud.

Dado que este diagrama es la representación de cinco iteraciones, los mejores hiperparámetros son aquellos que proporcionen el valor máximo. Pese a tener dos valores máximos para dos medidas distintas, los valores pertenecen a la misma instancia por lo que solo tenemos un grupo de hiperparámetros.

Para el caso de la lentitud los hiperparámetros fueron:
\begin{itemize}
\item \textit{grow\_policy}: \textit{depthwise}.
\item \textit{learning\_rate}: 0,1.
\item \textit{Number of estimators}: 200.
\end{itemize}
Este modelo con los hiperparámetros indicados obtuvo un \textit{F-score} de 0,69 y un \textit{G-mean} de 0,71 siendo estos los mejores valores reportados entre todos los modelos.

\subsection{Amplitud}

Al igual que en el caso de la lentitud,  se hicieron de forma paralela la evaluación del modelo para la lentitud utilizando ROS y SMOTE (ver figuras~\ref{fig:./graphs/comparativeAmpRos320.png} y~\ref{fig:./graphs/comparativeAmpSmote320.png} respectivamente).
\imagen{./graphs/comparativeAmpRos320.png}{Resultados de modelos con 320 características y ROS.}{}
\imagen{./graphs/comparativeAmpSmote320.png}{Resultados de modelos con 320 características y SMOTE.}{}

En el caso de la amplitud, la diferencia entre ROS y SMOTE no es tan pronunciada; sin embargo, podemos observar una diferencia notoria en el caso de \textit{XGBoost} el cual se ve claramente beneficiado por SMOTE. El resto de modelos, aunque no de forma tan notoria también se ven beneficiados por SMOTE. Es por ello que se decide tomar SMOTE como método de remuestreo como se hizo para la lentitud. 

Parecido a lo que ocurría con la lentitud KNN tiene peor desempeño en el caso de \textit{F-sore} y SVM, \textit{Random Forest} y \textit{XGBoost} tienen resultados muy parecidos. En el caso de \textit{G-mean} tanto SVM como \textit{Random Forest} presentan un peor rendimiento, mientras que los dos modelos restantes obtienen mejores resultados.

Dentro de las comparativas de SMOTE, vuelve a destacar \textit{XGBoost} que, en esta ocasión, posee un mejor rendimiento general mejor que el resto de modelos a diferencia de lo que ocurría con la lentitud que se eligió por el caso máximo. 

Como ya se explicó anteriormente al ser diagramas de cajas y bigotes, se elegirá el máximo ya que reporta los mejores rendimientos.

Para el caso de la amplitud los hiperparámetros fueron:
\begin{itemize}
\item \textit{grow\_policy}: \textit{depthwise}.
\item \textit{learning\_rate}: 0,1.
\item \textit{Number of estimators}: 800.
\end{itemize}
Este modelo con los hiperparámetros indicados obtuvo un \textit{F-score} de 0,69 y un \textit{G-mean} de 0,71 siendo estos los mejores valores reportados entre todos los modelos.

\section{Fase de desarrollo de la aplicación}
\label{Fase de desarrollo de la aplicación}
\subsection{Calidad el código}
Para evaluar y garantizar la calidad del código se hizo uso de la herramienta \textit{SonarCloud}. \textit{SonarCloud} es una plataforma de análisis de código y gestión de la calidad del software en la nube. Ofrece herramientas para detectar problemas en el código, tales como errores, vulnerabilidades, y código duplicado, y promueve prácticas de desarrollo de software de alta calidad.

Algunas de sus características principales son:
\begin{itemize}

\item \textbf{Análisis de Código}: Examina el código fuente para identificar errores y vulnerabilidades, asegurando que el software sea robusto y seguro.

\item \textbf{Integración Continua}: Se integra con herramientas de integración continua (CI) como Jenkins, GitHub Actions, Azure DevOps, y más, permitiendo análisis automáticos con cada commit o push al repositorio.

\item \textbf{Soporte Multilenguaje}: Soporta una amplia variedad de lenguajes de programación, incluyendo Java, JavaScript, TypeScript, C\#, C++, Python, PHP, y muchos más.

\item \textbf{Informes y Métricas}: Proporciona informes detallados y métricas sobre la calidad del código, facilitando el seguimiento del progreso y la identificación de áreas que necesitan mejora.
\end{itemize}

El uso de esta herramienta ha sido clave ya que ciertas vulnerabilidades que no se habían tenido en cuenta por desconocimiento de su solución o de las propias vulnerabilidades, fueron solucionadas de forma ágil ya que la propia herramienta muestra las soluciones y la ubicación de los errores.

\imagen{./sonarCloudReport.png}{Resumen del reporte de \textit{SonarCloud} sobre PDAIvolution.}{}

Como se observa en el reporte de \textit{SonarCloud} (ver figura~\ref{fig:./sonarCloudReport.png} existen varias categorías principales:
\begin{itemize}
\item \textit{\textbf{Security}}: muestra el número de problemas de seguridad que tiene la aplicación. Suelen tratarse de problemas como dejar visibles calves secretas, no tener en cuenta vulnerabilidades~\ldots En el caso del proyecto, no existe ningún problema de seguridad.
\item \textit{\textbf{Reliability}}: muestra el número de problemas de fiabilidad que tiene la aplicación. Se tratan de problemas como el uso de una variable antes de su declaración. En el caso del proyecto, no existe ningún problema de fiabilidad.
\item \textit{\textbf{Maintainability}}: muestra el número de problemas de mantenimiento que tiene la aplicación. En el caso del proyecto, no existe ningún problema de mantenimiento.
\item \textit{\textbf{Duplications}}: muestra el porcentaje de lineas duplicadas que tiene la aplicación. En el caso del proyecto, el porcentaje de lineas duplicadas es 0\%.
\item \textit{\textbf{Accepted Issues}}: muestra el número de problemas marcados como aceptados, es decir, problemas que se tiene conocimiento de ellos pero no se van a solucionar. En el caso del proyecto tiene marcados tres problemas como aceptados, dos de los cuales vienen dados por el proyecto PADDEL y el restante se debe a que la solución para el problema resulta más compleja que el problema en sí.
\item \textit{\textbf{Security Hotspots}}: son secciones del código que pueden no ser vulnerabilidades de seguridad por sí mismas, pero que merecen una revisión más detallada por parte de un desarrollador experimentado.
\end{itemize}

Por lo visto en el resumen, el proyecto, a excepción de los problemas previamente comentados, cumple los criterios de calidad de código.


\subsection{Problemas durante el desarrollo}

\subsubsection{Planificación temporal}

Durante el desarrollo de la aplicación, hubo dos problemas principales a la hora de realizar la parte de planificación temporal. El primero de ellos fue no conseguir conectar la aplicación de Zube con el repositorio de Github haciendo que este crease los \textit{issues} de forma automática por lo que se opto por indicar las tareas que se realizaban en cada uno de los \textit{commits}. El segundo problema se dio con el proyecto ya avanzado. Este problema fue la perdida total del proyecto en Zube. Debido a un fallo, que todavía se desconoce, el proyecto se eliminó por completo de la plataforma, eliminando a su vez todas las tareas de cada \textit{Sprint} así como la duración de los mismos. Este problema no se consiguió solucionar por lo que no se pudo recuperar nada de la información del proyecto.

\subsubsection{Compatibilidad}

Tanto durante la implementación del proyecto PADDEL como durante la fase de experimentación y despliegue, han habido problemas de compatibilidad.

Este problema de compatibilidad viene dado por la versión de CUDA~\footnote{Fuente:~\url{https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html}}y numba~\footnote{Fuente:~\url{https://numba.pydata.org/}}. CUDA (Compute Unified Device Architecture) es una plataforma de computación paralela y un modelo de programación creada por NVIDIA y numba es un compilador justo a tiempo (JIT) para Python que traduce funciones numéricas escritas en Python y NumPy a código máquina altamente optimizado utilizando el compilador LLVM. El problema se da cuando la versión de CUDA y numba no son compatibles entre sí. Cuando se da este problema, se ha de modificar las versiones de uno de los dos para que sean compatibles; sin embargo, al hacer la experimentación y despliegue en un servidor donde no se puede modificar la versión de CUDA, se termino resolviendo mediante la desactualización de la versión de numba provocando a su vez una desactualización de la versión de Python de la 3.12 a la 3.10. Es por este motivo por el que el proyecto no se encuentra en la versión más actualizada.

\subsection{Despliegue}

Por último, se destaca el despliegue en un servidor Linux real.
Primero se planteo el despliegue en un servicio gratuito pero, debido al gran tamaño de la aplicación y de los datos con los que se iba a trabajar, se descartó esta opción tras comprobar el reducido tamaño que ofrecen los servicios gratuitos. Tras esto, se opto por el despliegue en un servidor proporcionado por la Universidad de Burgos asegurando que la aplicación estuviera operativa y accesible de manera continua y confiable, pudiendo satisfacer las necesidades de uso. 
Durante la fase de desarrollo la aplicación fue probada en un entorno local haciendo uso de la herramienta integrada de Flask que permite lanzar un servidor de prueba y modificarlo en tiempo real. No obstante, a la hora del paso a producción, la forma en la que se despliega es mediante la herramienta de Gunicorn~\footnote{Fuente:~\url{https://gunicorn.org/}}. Gunicorn \textit{Green Unicorn} es un servidor HTTP Python WSGI para UNIX.

A su vez se creo una imagen de \textit{DockerHub}~\footnote{Fuente:~\url{https://hub.docker.com/}} (disponible en~\url{https://hub.docker.com/repository/docker/jornez/pdaivolution/general} que permite hacer un despliegue muy sencillo sin necesidad de instalar nada a excepción de \textit{Docker} o \textit{Docker Desktop}.



