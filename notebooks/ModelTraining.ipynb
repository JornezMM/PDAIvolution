{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from itertools import product\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameter_rules = {\n",
    "    SVC: [\n",
    "        (\n",
    "            {},\n",
    "            {\n",
    "                \"C\": [0.5, 0.75, 1, 1.5],\n",
    "                \"kernel\": [\"linear\"],\n",
    "                \"class_weight\": [\"balanced\"],\n",
    "            },\n",
    "        ),\n",
    "        ({\"kernel\":{\"poly\"}}, {\"degree\": [2, 3, 4]}), \n",
    "        ({\"kernel\": {\"rbf\", \"sigmoid\"}}, {\"gamma\": [\"auto\"]}),\n",
    "    ],\n",
    "    KNeighborsClassifier: [\n",
    "        (\n",
    "            {},\n",
    "            {\n",
    "                \"n_neighbors\": [3, 5, 7, 9, 11, 15],\n",
    "                \"weights\": [\"uniform\", \"distance\"],\n",
    "                \"metric\": [\n",
    "                    \"cityblock\",\n",
    "                    \"cosine\",\n",
    "                    \"l1\",\n",
    "                    \"l2\",\n",
    "                    \"nan_euclidean\",\n",
    "                ],\n",
    "            },\n",
    "        ),\n",
    "        ({\"metric\": [\"minkowski\"]}, {\"p\": [1, 2, 3, 4]}),\n",
    "    ],\n",
    "    RandomForestClassifier: [\n",
    "        (\n",
    "            {},\n",
    "            {\n",
    "                \"n_estimators\": [50, 100, 200, 400, 800, 1600],\n",
    "                \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "                \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "            },\n",
    "        )\n",
    "    ],\n",
    "    XGBClassifier: [\n",
    "        (\n",
    "            {},\n",
    "            {\n",
    "                \"n_estimators\": [25, 50, 100, 200, 400, 800],\n",
    "                \"grow_policy\": [\"depthwise\", \"lossguide\"],\n",
    "                \"learning_rate\": [0.01, 0.1, 1],\n",
    "            },\n",
    "        )\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracted code from PADDEL library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashableDict(dict):\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(sorted(self.items())))\n",
    "\n",
    "\n",
    "def expand_rules(parameter_rules: list) -> dict:\n",
    "    \"\"\"Converts list of conditions and options to a dictionary that is easier to\n",
    "    parse.\n",
    "\n",
    "    Args:\n",
    "        parameter_rules (list): Rules to expand.\n",
    "\n",
    "    Returns:\n",
    "        dict: Expanded rules.\n",
    "    \"\"\"\n",
    "    expanded_rules = {}\n",
    "\n",
    "    for conditions, parameters in parameter_rules:\n",
    "        for values in product(*conditions.values()):\n",
    "            simple_conditions = HashableDict(\n",
    "                zip(conditions.keys(), [tuple([v]) for v in values])\n",
    "            )\n",
    "\n",
    "            if simple_conditions not in expanded_rules:\n",
    "                expanded_rules[simple_conditions] = {}\n",
    "\n",
    "            expanded_rules[simple_conditions].update(parameters)\n",
    "\n",
    "    return expanded_rules\n",
    "\n",
    "\n",
    "def matches(conditions: dict, other_conditions: dict) -> bool:\n",
    "    \"\"\"Determines if a condition dictionary matches within another.\n",
    "\n",
    "    Args:\n",
    "        conditions (dict): Conditions to match.\n",
    "        other_conditions (dict): Conditions to match with.\n",
    "\n",
    "    Returns:\n",
    "        bool: If condition matches.\n",
    "    \"\"\"\n",
    "    return conditions.items() > other_conditions.items()\n",
    "\n",
    "\n",
    "def merge_parameters(one: dict, other: dict) -> dict:\n",
    "    \"\"\"Merge parameters from one dict to another.\n",
    "\n",
    "    Args:\n",
    "        one (dict): Dict to merge into.\n",
    "        other (dict): Dict to merge from.\n",
    "\n",
    "    Returns:\n",
    "        dict: Merged dictionary.\n",
    "    \"\"\"\n",
    "    one = one.copy()\n",
    "    for key in other:\n",
    "        if key in one:\n",
    "            one[key] += other[key]\n",
    "        else:\n",
    "            one[key] = other[key]\n",
    "\n",
    "    return one\n",
    "\n",
    "\n",
    "def parse_hyper_parameters(parameter_rules: list, prefix=\"\") -> list:\n",
    "    \"\"\"Parses custom formatted parameter rules to sklearn compatible parameter grid.\n",
    "\n",
    "    Args:\n",
    "        parameter_rules (list): Parameter rules.\n",
    "        prefix (str, optional): Prefix to be used when naming the parameters. Useful when working with pipelines. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        list: Parameter grid.\n",
    "    \"\"\"\n",
    "    parameter_rules = expand_rules(parameter_rules)\n",
    "\n",
    "    param_grid = []\n",
    "\n",
    "    for conditions in parameter_rules:\n",
    "        parameters = parameter_rules[conditions].copy()\n",
    "        for other_conditions in parameter_rules:\n",
    "            if matches(conditions, other_conditions):\n",
    "                parameters = merge_parameters(\n",
    "                    parameters, parameter_rules[other_conditions]\n",
    "                )\n",
    "\n",
    "        parameters.update({k: list(v) for k, v in conditions.items()})\n",
    "\n",
    "        param_grid.append(parameters)\n",
    "\n",
    "    # Rename to match model if using pipeline\n",
    "    for params in param_grid:\n",
    "        for key in list(params):\n",
    "            params[f\"{prefix}{key}\"] = params.pop(key)\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load and concatenate multiple CSV files into a single DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    data (pandas.DataFrame): The concatenated DataFrame containing data from misc_df, classic_df, and fresh_df.\n",
    "    \"\"\"\n",
    "    misc_df = pd.read_csv('misc_df.csv')\n",
    "    classic_df = pd.read_csv('classic_df.csv')\n",
    "    fresh_df = pd.read_csv('fresh_df.csv')\n",
    "    data = pd.concat([misc_df, classic_df, fresh_df], axis=1)\n",
    "    return data\n",
    "\n",
    "data = load_data()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_value_counts():\n",
    "    \"\"\"\n",
    "    Prints the value counts for the 'lent' and 'amp' columns in the 'data' DataFrame.\n",
    "    \"\"\"\n",
    "    value_counts_lent = data['lent'].value_counts()\n",
    "    value_counts_amp = data['amp'].value_counts()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    ax[0].bar(value_counts_lent.index, value_counts_lent.values)\n",
    "    ax[0].set_xlabel('Class')\n",
    "    ax[0].set_ylabel('Instances')\n",
    "    ax[0].set_title('Number of instances per class in \"lent\"')\n",
    "\n",
    "    ax[1].bar(value_counts_amp.index, value_counts_amp.values)\n",
    "    ax[1].set_xlabel('Class')\n",
    "    ax[1].set_ylabel('Instances')\n",
    "    ax[1].set_title('Number of instances per class in \"amp\"')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Preprocesses the given data by removing rows and columns based on certain conditions.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input data to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The preprocessed data.\n",
    "    \"\"\"\n",
    "    index_to_remove = data[(data['lent'] == 4) | (data['amp'] == 4) | (data['age'] == 'XX')].index\n",
    "    data = data.drop(index_to_remove)\n",
    "    data = data[data['detection_time'] >= 15]\n",
    "    columns_to_remove = ['sample_name', 'date', 'video_path']\n",
    "    data = data.drop(columns=columns_to_remove)\n",
    "    return data\n",
    "data=preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_remove=data[(data['lent']==3)].index\n",
    "data=data.drop(index_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data):\n",
    "    \"\"\"\n",
    "    Performs label encoding and data type conversion.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input data to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The preprocessed data.\n",
    "    \"\"\"\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['hand'] = label_encoder.fit_transform(data['hand'])\n",
    "    data['gender'] = label_encoder.fit_transform(data['gender'])\n",
    "    data['handedness'] = label_encoder.fit_transform(data['handedness'])\n",
    "    data = data.drop(columns=\"angle__query_similarity_count__query_None__threshold_0.0\")\n",
    "    data['age'] = data['age'].astype(int)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = encode_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slowness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_folds=5\n",
    "random_state=42\n",
    "k_neighbors=3\n",
    "lent_data=data.drop(columns=['amp'])\n",
    "X=data.drop(columns=['lent'])\n",
    "y=data['lent']\n",
    "all_results_smote_lent=[]\n",
    "all_results_ros_lent=[]\n",
    "models = {\n",
    "    \"svm_linear\": SVC,\n",
    "    \"xgboost\":XGBClassifier,\n",
    "    \"knn\": KNeighborsClassifier,\n",
    "    \"rf\": RandomForestClassifier\n",
    "}\n",
    "smote = SMOTE(random_state=random_state,k_neighbors=k_neighbors)\n",
    "ros = RandomOverSampler(random_state=random_state)\n",
    "skf=StratifiedKFold(n_splits=n_folds)\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "f1_scorer = make_scorer(f1_score, average=\"weighted\")\n",
    "gmean_scorer = make_scorer(geometric_mean_score, average='weighted')\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    SelectKBest(k=320).fit_transform(X_train, y_train)\n",
    "    X_res, y_res = ros.fit_resample(X_train, y_train)\n",
    "    X_res_s, y_res_s = smote.fit_resample(X_train, y_train)\n",
    "    for model_name, model in models.items():\n",
    "        model_param_grid = parse_hyper_parameters(model_parameter_rules[model])\n",
    "        grid = GridSearchCV(\n",
    "            estimator=model(),\n",
    "            param_grid=model_param_grid,\n",
    "            scoring={\n",
    "                \"g_mean\":gmean_scorer,\n",
    "            },\n",
    "            refit=\"g_mean\",\n",
    "            n_jobs=10,\n",
    "            cv=StratifiedKFold(n_splits=2),\n",
    "            verbose=0\n",
    "        )\n",
    "        print(f\"Doing dataset: full, model: {model_name}, features: 320\")\n",
    "        grid.fit(X_res, y_res)\n",
    "        best_estimator = grid.best_estimator_\n",
    "        accuracy = accuracy_scorer(best_estimator,X_test, y_test)\n",
    "        f1 = f1_scorer(best_estimator,X_test, y_test)\n",
    "        gmean = gmean_scorer(best_estimator, X_test, y_test)\n",
    "        \n",
    "        all_results_ros_lent.append({\n",
    "            \"dataset\": \"full\",\n",
    "            \"model\": model_name,\n",
    "            \"features\": 320,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1\": f1,\n",
    "            \"gmean\": gmean,\n",
    "            \"parameters\": grid.best_params_\n",
    "        })\n",
    "        grid.fit(X_res_s, y_res_s)\n",
    "        best_estimator = grid.best_estimator_\n",
    "        accuracy = accuracy_scorer(best_estimator,X_test, y_test)\n",
    "        f1 = f1_scorer(best_estimator,X_test, y_test)\n",
    "        gmean = gmean_scorer(best_estimator, X_test, y_test)\n",
    "        all_results_smote_lent.append({\n",
    "            \"dataset\": \"full\",\n",
    "            \"model\": model_name,\n",
    "            \"features\": 320,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1\": f1,\n",
    "            \"gmean\": gmean,\n",
    "            \"parameters\": grid.best_params_\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_results_smote_lent)\n",
    "df.to_csv(\"./all_results_smote_lent.csv\", index=False)\n",
    "df = pd.DataFrame(all_results_ros_lent)\n",
    "df.to_csv(\"./all_results_ros_lent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=5\n",
    "random_state=42\n",
    "k_neighbors=3\n",
    "lent_data=data.drop(columns=['lent'])\n",
    "X=data.drop(columns=['amp'])\n",
    "y=data['amp']\n",
    "all_results_smote_amp=[]\n",
    "all_results_ros_amp=[]\n",
    "models = {\n",
    "    \"svm_linear\": SVC,\n",
    "    \"xgboost\":XGBClassifier,\n",
    "    \"knn\": KNeighborsClassifier,\n",
    "    \"rf\": RandomForestClassifier\n",
    "}\n",
    "smote = SMOTE(random_state=random_state,k_neighbors=k_neighbors)\n",
    "ros = RandomOverSampler(random_state=random_state)\n",
    "skf=StratifiedKFold(n_splits=n_folds)\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "f1_scorer = make_scorer(f1_score, average=\"weighted\")\n",
    "gmean_scorer = make_scorer(geometric_mean_score, average='weighted')\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    SelectKBest(k=320).fit_transform(X_train, y_train)\n",
    "    X_res, y_res = ros.fit_resample(X_train, y_train)\n",
    "    X_res_s, y_res_s = smote.fit_resample(X_train, y_train)\n",
    "    for model_name, model in models.items():\n",
    "        model_param_grid = parse_hyper_parameters(model_parameter_rules[model])\n",
    "        grid = GridSearchCV(\n",
    "            estimator=model(),\n",
    "            param_grid=model_param_grid,\n",
    "            scoring={\n",
    "                \"g_mean\":gmean_scorer,\n",
    "            },\n",
    "            refit=\"g_mean\",\n",
    "            n_jobs=10,\n",
    "            cv=StratifiedKFold(n_splits=2),\n",
    "            verbose=0\n",
    "        )\n",
    "        print(f\"Doing dataset: full, model: {model_name}, features: 320\")\n",
    "        grid.fit(X_res, y_res)\n",
    "        best_estimator = grid.best_estimator_\n",
    "        accuracy = accuracy_scorer(best_estimator,X_test, y_test)\n",
    "        f1 = f1_scorer(best_estimator,X_test, y_test)\n",
    "        gmean = gmean_scorer(best_estimator, X_test, y_test)\n",
    "        \n",
    "        all_results_ros_amp.append({\n",
    "            \"dataset\": \"full\",\n",
    "            \"model\": model_name,\n",
    "            \"features\": 320,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1\": f1,\n",
    "            \"gmean\": gmean,\n",
    "            \"parameters\": grid.best_params_\n",
    "        })\n",
    "        grid.fit(X_res_s, y_res_s)\n",
    "        best_estimator = grid.best_estimator_\n",
    "        accuracy = accuracy_scorer(best_estimator,X_test, y_test)\n",
    "        f1 = f1_scorer(best_estimator,X_test, y_test)\n",
    "        gmean = gmean_scorer(best_estimator, X_test, y_test)\n",
    "        all_results_smote_amp.append({\n",
    "            \"dataset\": \"full\",\n",
    "            \"model\": model_name,\n",
    "            \"features\": 320,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1\": f1,\n",
    "            \"gmean\": gmean,\n",
    "            \"parameters\": grid.best_params_\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_results_smote_amp)\n",
    "df.to_csv(\"./all_results_smote_amp.csv\", index=False)\n",
    "df = pd.DataFrame(all_results_ros_amp)\n",
    "df.to_csv(\"./all_results_ros_amp.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
